{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aef933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70929225",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7daec114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3654c52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f367cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8797162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{']', 'S', 'k', '-', '\"', 'I', ';', 'a', 'h', '`', 'G', 'L', 'T', 's', '|', 'p', 'f', '1', 'y', '9', 'Z', 'R', '(', 'd', '4', '[', 'c', '2', 'm', '<', 'g', '!', 'r', ' ', 'U', 'q', '_', 'A', 'o', 'E', \"'\", 'Y', 'u', 'x', 'F', 'J', '>', 'D', ')', '&', 'w', 't', 'v', 'i', 'l', 'W', '.', 'C', 'e', 'P', 'X', '6', ',', 'O', 'Q', '}', '\\n', 'H', 'N', ':', 'b', '0', '5', '?', 'M', 'n', '8', 'z', '7', 'K', 'B', 'V', 'j', '3'}\n"
     ]
    }
   ],
   "source": [
    "all_characters = set(text)\n",
    "print(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3e1f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "print(len(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7c0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ']', 1: 'S', 2: 'k', 3: '-', 4: '\"', 5: 'I', 6: ';', 7: 'a', 8: 'h', 9: '`', 10: 'G', 11: 'L', 12: 'T', 13: 's', 14: '|', 15: 'p', 16: 'f', 17: '1', 18: 'y', 19: '9', 20: 'Z', 21: 'R', 22: '(', 23: 'd', 24: '4', 25: '[', 26: 'c', 27: '2', 28: 'm', 29: '<', 30: 'g', 31: '!', 32: 'r', 33: ' ', 34: 'U', 35: 'q', 36: '_', 37: 'A', 38: 'o', 39: 'E', 40: \"'\", 41: 'Y', 42: 'u', 43: 'x', 44: 'F', 45: 'J', 46: '>', 47: 'D', 48: ')', 49: '&', 50: 'w', 51: 't', 52: 'v', 53: 'i', 54: 'l', 55: 'W', 56: '.', 57: 'C', 58: 'e', 59: 'P', 60: 'X', 61: '6', 62: ',', 63: 'O', 64: 'Q', 65: '}', 66: '\\n', 67: 'H', 68: 'N', 69: ':', 70: 'b', 71: '0', 72: '5', 73: '?', 74: 'M', 75: 'n', 76: '8', 77: 'z', 78: '7', 79: 'K', 80: 'B', 81: 'V', 82: 'j', 83: '3'}\n"
     ]
    }
   ],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3dfe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ']')\n",
      "(1, 'S')\n",
      "(2, 'k')\n",
      "(3, '-')\n",
      "(4, '\"')\n",
      "(5, 'I')\n",
      "(6, ';')\n",
      "(7, 'a')\n",
      "(8, 'h')\n",
      "(9, '`')\n",
      "(10, 'G')\n",
      "(11, 'L')\n",
      "(12, 'T')\n",
      "(13, 's')\n",
      "(14, '|')\n",
      "(15, 'p')\n",
      "(16, 'f')\n",
      "(17, '1')\n",
      "(18, 'y')\n",
      "(19, '9')\n",
      "(20, 'Z')\n",
      "(21, 'R')\n",
      "(22, '(')\n",
      "(23, 'd')\n",
      "(24, '4')\n",
      "(25, '[')\n",
      "(26, 'c')\n",
      "(27, '2')\n",
      "(28, 'm')\n",
      "(29, '<')\n",
      "(30, 'g')\n",
      "(31, '!')\n",
      "(32, 'r')\n",
      "(33, ' ')\n",
      "(34, 'U')\n",
      "(35, 'q')\n",
      "(36, '_')\n",
      "(37, 'A')\n",
      "(38, 'o')\n",
      "(39, 'E')\n",
      "(40, \"'\")\n",
      "(41, 'Y')\n",
      "(42, 'u')\n",
      "(43, 'x')\n",
      "(44, 'F')\n",
      "(45, 'J')\n",
      "(46, '>')\n",
      "(47, 'D')\n",
      "(48, ')')\n",
      "(49, '&')\n",
      "(50, 'w')\n",
      "(51, 't')\n",
      "(52, 'v')\n",
      "(53, 'i')\n",
      "(54, 'l')\n",
      "(55, 'W')\n",
      "(56, '.')\n",
      "(57, 'C')\n",
      "(58, 'e')\n",
      "(59, 'P')\n",
      "(60, 'X')\n",
      "(61, '6')\n",
      "(62, ',')\n",
      "(63, 'O')\n",
      "(64, 'Q')\n",
      "(65, '}')\n",
      "(66, '\\n')\n",
      "(67, 'H')\n",
      "(68, 'N')\n",
      "(69, ':')\n",
      "(70, 'b')\n",
      "(71, '0')\n",
      "(72, '5')\n",
      "(73, '?')\n",
      "(74, 'M')\n",
      "(75, 'n')\n",
      "(76, '8')\n",
      "(77, 'z')\n",
      "(78, '7')\n",
      "(79, 'K')\n",
      "(80, 'B')\n",
      "(81, 'V')\n",
      "(82, 'j')\n",
      "(83, '3')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(all_characters):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624140ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{']': 0, 'S': 1, 'k': 2, '-': 3, '\"': 4, 'I': 5, ';': 6, 'a': 7, 'h': 8, '`': 9, 'G': 10, 'L': 11, 'T': 12, 's': 13, '|': 14, 'p': 15, 'f': 16, '1': 17, 'y': 18, '9': 19, 'Z': 20, 'R': 21, '(': 22, 'd': 23, '4': 24, '[': 25, 'c': 26, '2': 27, 'm': 28, '<': 29, 'g': 30, '!': 31, 'r': 32, ' ': 33, 'U': 34, 'q': 35, '_': 36, 'A': 37, 'o': 38, 'E': 39, \"'\": 40, 'Y': 41, 'u': 42, 'x': 43, 'F': 44, 'J': 45, '>': 46, 'D': 47, ')': 48, '&': 49, 'w': 50, 't': 51, 'v': 52, 'i': 53, 'l': 54, 'W': 55, '.': 56, 'C': 57, 'e': 58, 'P': 59, 'X': 60, '6': 61, ',': 62, 'O': 63, 'Q': 64, '}': 65, '\\n': 66, 'H': 67, 'N': 68, ':': 69, 'b': 70, '0': 71, '5': 72, '?': 73, 'M': 74, 'n': 75, '8': 76, 'z': 77, '7': 78, 'K': 79, 'B': 80, 'V': 81, 'j': 82, '3': 83}\n"
     ]
    }
   ],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295acf8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 17 66\n",
      " 33 33 44 32 38 28 33 16  7 53 32 58 13 51 33 26 32 58  7 51 42 32 58 13\n",
      " 33 50 58 33 23 58 13 53 32 58 33 53 75 26 32 58  7 13 58 62 66 33 33 12\n",
      "  8  7 51 33 51  8 58 32 58 70 18 33 70 58  7 42 51 18 40 13 33 32 38 13\n",
      " 58 33 28 53]\n"
     ]
    }
   ],
   "source": [
    "# ENCODING THE TEXT\n",
    "\n",
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "print(encoded_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c07c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c38720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING\n",
    "def one_hot_encoder(encoded_text,num_uni_chars):\n",
    "    \"\"\"\n",
    "    encoded_text: batch of encoded text\n",
    "    number_uni_chars : len(set(text))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a placeholder array\n",
    "    one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
    "    \n",
    "    one_hot = one_hot.astype(np.float32)  # PyTorch needed\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
    "    \n",
    "    \n",
    "    one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae92ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c90b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(array,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9727866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "example_text = np.arange(10)\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6008d15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a31cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text,samp_per_batch=10,seq_len=50):\n",
    "    # Use a generator, to save memory\n",
    "    # X: encoded text of length seq_len\n",
    "    # Y: encoded text shifted by one\n",
    "    \n",
    "    # Number of characters per batch\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    # how many batches can we make, given the len of encoded text\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch) # number of batches to make\n",
    "    \n",
    "    # Cut off the end of the encoded text that won't fit evenly in the batches\n",
    "    # Leads to a slight loss of information\n",
    "    encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
    "    \n",
    "    # Reshape the text into rows the size of the batch\n",
    "    \n",
    "    encoded_text = encoded_text.reshape((samp_per_batch,-1))\n",
    "    \n",
    "    for n in range(0,encoded_text.shape[1],seq_len):\n",
    "        \n",
    "        x = encoded_text[:,n:n+seq_len]\n",
    "        \n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,n+seq_len]\n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:-1] = encoded_text[:,0]\n",
    "        \n",
    "        yield x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fcf75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aefcfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33]\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877d7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc02334",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c99f8825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66, 33, 33, 33, 33],\n",
       "       [33, 33, 33, 33, 33]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8ba3a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33, 33, 33, 33, 33],\n",
       "       [33, 33, 33, 33, 33]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4126420",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = np.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39f356ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [10 11 12 13 14]]\n",
      "[[ 1  2  3  4  5]\n",
      " [11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "len(sample_text)\n",
    "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)\n",
    "x,y = next(batch_generator)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c5b683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ef0305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharModel(all_chars=all_characters,num_hidden=512,num_layers=3,drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "619ad0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172032\n",
      "1048576\n",
      "2048\n",
      "2048\n",
      "1048576\n",
      "1048576\n",
      "2048\n",
      "2048\n",
      "1048576\n",
      "1048576\n",
      "2048\n",
      "2048\n",
      "43008\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "total_param = []\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param.numel())\n",
    "    total_param.append(int(param.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c6d134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e773f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bb53cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf27540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1683cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a091a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = encoded_text[train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5e9854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "seq_len = 100\n",
    "tracker = 0\n",
    "\n",
    "num_char = max(encoded_text+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a87e344",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14748\\1364059124.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_hot_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14748\\3197583017.py\u001b[0m in \u001b[0;36mgenerate_batches\u001b[1;34m(encoded_text, samp_per_batch, seq_len)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# how many batches can we make, given the len of encoded text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mnum_batches_avail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mchar_per_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# number of batches to make\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Cut off the end of the encoded text that won't fit evenly in the batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.int32' has no len()"
     ]
    }
   ],
   "source": [
    "## VARIABLES\n",
    "\n",
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1\n",
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
